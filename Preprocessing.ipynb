{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a4b279",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32550dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3a870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('my_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7879315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name    city  gender profession   age  cgpa  placed\n",
      "0  moriarity     NaN  female        phd  28.0  5.94       1\n",
      "1  moriarity  asgard     NaN   bachelor  50.0  8.55       0\n",
      "2     holmes     NaN  female    masters  18.0  5.56       0\n",
      "3        sam     NaN    male   bachelor  25.0  8.57       1\n",
      "4        sam     NaN    male   bachelor  19.0  8.76       1\n",
      "(1100, 7)\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3ff3695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1100 entries, 0 to 1099\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   name        1009 non-null   object \n",
      " 1   city        905 non-null    object \n",
      " 2   gender      994 non-null    object \n",
      " 3   profession  921 non-null    object \n",
      " 4   age         982 non-null    float64\n",
      " 5   cgpa        962 non-null    float64\n",
      " 6   placed      1100 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 60.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29233235",
   "metadata": {},
   "source": [
    "Creating a independent(y) and dependent variable(X)\n",
    "dependent variable are those in which outcomes depended\n",
    "indenpendent variables are those on which outcomes are not depende(outcome itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "306ba707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iloc index location , ['starting row':'ending row','starting column':'ending column']\n",
    "X=data.iloc[:,:-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9784c957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['moriarity', nan, 'female', 'phd', 28.0, 5.94],\n",
       "       ['moriarity', 'asgard', nan, 'bachelor', 50.0, 8.55],\n",
       "       ['holmes', nan, 'female', 'masters', 18.0, 5.56],\n",
       "       ...,\n",
       "       ['sam', 'asgard', 'male', 'bachelor', 30.0, 7.88],\n",
       "       ['dean', 'gotham', 'male', 'masters', 28.0, nan],\n",
       "       ['sam', 'asgard', nan, 'masters', 24.0, 8.94]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aea36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4073a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a1a8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to handle missing data by filling with mean of that column that has numerical data\n",
    "from sklearn.impute import SimpleImputer\n",
    "SimpleImputer=SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "SimpleImputer=SimpleImputer.fit(X[:,4:6]) #fit only on columns which have missing data\n",
    "X[:,4:6]=SimpleImputer.transform(X[:,4:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba36fab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['moriarity' nan 'female' 'phd' 28.0 5.94]\n",
      " ['moriarity' 'asgard' nan 'bachelor' 50.0 8.55]\n",
      " ['holmes' nan 'female' 'masters' 18.0 5.56]\n",
      " ...\n",
      " ['sam' 'asgard' 'male' 'bachelor' 30.0 7.88]\n",
      " ['dean' 'gotham' 'male' 'masters' 28.0 7.609033264033264]\n",
      " ['sam' 'asgard' nan 'masters' 24.0 8.94]]\n",
      "---***------***------***------***------***------***---\n",
      "---***------***------***------***------***------***---\n",
      "---***------***------***------***------***------***---\n",
      "[['moriarity' 'wakanda' 'female' 'phd' 28.0 5.94]\n",
      " ['moriarity' 'asgard' 'male' 'bachelor' 50.0 8.55]\n",
      " ['holmes' 'wakanda' 'female' 'masters' 18.0 5.56]\n",
      " ...\n",
      " ['sam' 'asgard' 'male' 'bachelor' 30.0 7.88]\n",
      " ['dean' 'gotham' 'male' 'masters' 28.0 7.609033264033264]\n",
      " ['sam' 'asgard' 'male' 'masters' 24.0 8.94]]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "\n",
    "#How to handle non numerical data(categorical data) missi ng data by filling with most frequent value of that column\n",
    "from sklearn.impute import SimpleImputer\n",
    "SimpleImputer=SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n",
    "SimpleImputer=SimpleImputer.fit(X[:,1:4]) #fit only on columns which have missing data\n",
    "X[:,1:4]=SimpleImputer.transform(X[:,1:4])\n",
    "\n",
    "print('---***---'*6)\n",
    "print('---***---'*6)\n",
    "print('---***---'*6)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560eaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to delete rows with missing data\n",
    "#data.dropna(inplace=True)\n",
    "#to delete columns with missing data\n",
    "#data.dropna(axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c88f8f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city\n",
       "wakanda       391\n",
       "gotham        243\n",
       "asgard        152\n",
       "purgatory     119\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['city'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7749f2f",
   "metadata": {},
   "source": [
    "Now suppose the city column is important for our model training, we will have to convert the string data to interger / numbeeric type for machine to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14623f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['moriarity' 3 'female' 'phd' 28.0 5.94]\n",
      " ['moriarity' 0 'male' 'bachelor' 50.0 8.55]\n",
      " ['holmes' 3 'female' 'masters' 18.0 5.56]\n",
      " ...\n",
      " ['sam' 0 'male' 'bachelor' 30.0 7.88]\n",
      " ['dean' 1 'male' 'masters' 28.0 7.609033264033264]\n",
      " ['sam' 0 'male' 'masters' 24.0 8.94]]\n",
      "Number of different type of city in data 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X=LabelEncoder()\n",
    "X[:,1]=labelencoder_X.fit_transform(X[:,1])\n",
    "print(X)\n",
    "print(\"Number of different type of city in data\",data['city'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d94086",
   "metadata": {},
   "source": [
    "Now if we see the city column has been converted in number type 0,1,2,3 but this could be misunderstood by machine considereing 1 as higher presidence then 0 so we will create dummy metrix.\n",
    "\n",
    "In machine learning, a \"dummy matrix\" usually refers to a matrix of dummy variables, which are binary (0/1) indicators used to represent categorical data numerically.\n",
    "\n",
    "data   | wakanda |  gotham |  asgard |  purgatory\n",
    "\n",
    "Wakanda   |  1    |    0   |     0    |    0\n",
    "gotham    |  0    |    1   |     0    |    0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f615f522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['moriarity' 0.0 0.0 ... 'phd' 28.0 5.94]\n",
      " ['moriarity' 1.0 0.0 ... 'bachelor' 50.0 8.55]\n",
      " ['holmes' 0.0 0.0 ... 'masters' 18.0 5.56]\n",
      " ...\n",
      " ['sam' 1.0 0.0 ... 'bachelor' 30.0 7.88]\n",
      " ['dean' 0.0 1.0 ... 'masters' 28.0 7.609033264033264]\n",
      " ['sam' 1.0 0.0 ... 'masters' 24.0 8.94]]\n",
      "[['moriarity' 0.0 0.0 ... 'phd' 28.0 5.94]\n",
      " ['moriarity' 1.0 0.0 ... 'bachelor' 50.0 8.55]\n",
      " ['holmes' 0.0 0.0 ... 'masters' 18.0 5.56]\n",
      " ...\n",
      " ['sam' 1.0 0.0 ... 'bachelor' 30.0 7.88]\n",
      " ['dean' 0.0 1.0 ... 'masters' 28.0 7.609033264033264]\n",
      " ['sam' 1.0 0.0 ... 'masters' 24.0 8.94]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amarj\\Python_env_Scripts\\Deep_learning_env\\tf_env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder=OneHotEncoder(sparse=False)  # sparse=False returns a dense array\n",
    "\n",
    "# Apply encoder only to the categorical column (column index 1)\n",
    "\n",
    "X_encoded=encoder.fit_transform((X[:,[1]]))\n",
    "\n",
    "# Combine encoded column with the rest of the data\n",
    "\n",
    "X_final=np.concatenate((X[:,0:1],X_encoded,X[:,2:]),axis=1)\n",
    "\n",
    "X=X_final\n",
    "np.set_printoptions(suppress=True)\n",
    "print(X)\n",
    "\n",
    "\"\"\"OneHotEncoder=OneHotEncoder(categorical_features=[1])\n",
    "X=OneHotEncoder.fit_transform(X).toarray()\n",
    "np.set_printoptions(suppress=True)\"\"\"\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a7900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['watson' 1.0 0.0 ... 'masters' 27.0 7.79]\n",
      " ['sherlock' 0.0 0.0 ... 'bachelor' 49.0 9.38]\n",
      " ['sam' 1.0 0.0 ... 'bachelor' 19.0 7.44]\n",
      " ...\n",
      " ['dean' 0.0 1.0 ... 'masters' 22.0 9.22]\n",
      " ['sam' 0.0 0.0 ... 'masters' 29.0 8.48]\n",
      " ['sam' 0.0 0.0 ... 'masters' 24.0 9.21]]\n",
      "[['sam' 0.0 0.0 ... 'bachelor' 27.0 8.36]\n",
      " ['sam' 1.0 0.0 ... 'bachelor' 22.0 9.4]\n",
      " ['sam' 0.0 0.0 ... 'bachelor' 45.0 7.17]\n",
      " ...\n",
      " ['sam' 0.0 1.0 ... 'bachelor' 28.429735234215887 9.27]\n",
      " ['moriarity' 0.0 0.0 ... 'bachelor' 23.0 8.26]\n",
      " ['sam' 0.0 0.0 ... 'bachelor' 21.0 4.59]]\n",
      "[1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0\n",
      " 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0\n",
      " 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1\n",
      " 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0\n",
      " 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0\n",
      " 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1\n",
      " 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0\n",
      " 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
      " 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1\n",
      " 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0\n",
      " 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
      " 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1\n",
      " 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1\n",
      " 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0\n",
      " 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0\n",
      " 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
      " 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0\n",
      " 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1\n",
      " 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1]\n",
      "[1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1\n",
      " 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
      " 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1\n",
      " 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1\n",
      " 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#splitting the dataset into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0) \n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)      \n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6af28f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'watson'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      3\u001b[0m Scale\u001b[38;5;241m=\u001b[39mStandardScaler()\n\u001b[1;32m----> 4\u001b[0m X_train\u001b[38;5;241m=\u001b[39m\u001b[43mScale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m X_test\u001b[38;5;241m=\u001b[39mScale\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train)\n",
      "File \u001b[1;32mc:\\Users\\amarj\\Python_env_Scripts\\Deep_learning_env\\tf_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\amarj\\Python_env_Scripts\\Deep_learning_env\\tf_env\\lib\\site-packages\\sklearn\\base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\amarj\\Python_env_Scripts\\Deep_learning_env\\tf_env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:839\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amarj\\Python_env_Scripts\\Deep_learning_env\\tf_env\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amarj\\Python_env_Scripts\\Deep_learning_env\\tf_env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:875\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \n\u001b[0;32m    845\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    874\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 875\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\amarj\\Python_env_Scripts\\Deep_learning_env\\tf_env\\lib\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\amarj\\Python_env_Scripts\\Deep_learning_env\\tf_env\\lib\\site-packages\\sklearn\\utils\\validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    913\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 915\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    919\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amarj\\Python_env_Scripts\\Deep_learning_env\\tf_env\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'watson'"
     ]
    }
   ],
   "source": [
    "#Feature Scaling i.e standard deviation and normalization i.e handling varying data(big and small ) numbers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Scale=StandardScaler()\n",
    "X_train=Scale.fit_transform(X_train)\n",
    "X_test=Scale.transform(X_test)\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e20be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
